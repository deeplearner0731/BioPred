)
library(usethis)
usethis::use_vignette("Tutorial")
rm(list=ls())
library(BioPred)
library(BioPred)
unlink("vignettes/Tutorial_cache", recursive = TRUE)
rm(list=ls())
library(BioPred)
library(ggplot2)
library(xgboost)
library(survival)
library(survminer)
head(tutorial_data)
# Prepare the data
X_feature=tutorial_data[,c("x1", "x2" ,"x3" ,"x4","x5","x6","x7","x8","x9","x10")]
y_label=tutorial_data$y.time
delta=tutorial_data$y.event
# Convert treatment variable to (1 -1) format (1 for treatment, -1 for control)
trt=ifelse(tutorial_data$treatment==1, 1, -1)
# Estimate the propensity score using logistic regression
data_logit=tutorial_data[,c("treatment","x1", "x2" ,"x3" ,"x4" , "x5", "x6" , "x7","x8","x9","x10")]
logit_model <- glm(treatment~ ., data = data_logit, family = binomial)
pi <- predict(logit_model, type = "response")
# Train the XGBoostSub_sur model using the specified parameters
model <- XGBoostSub_sur(X_feature, y_label, trt ,pi,delta,Loss_type = "Weight_learning", params=list(learning_rate = 0.005, max_depth = 1,lambda = 9,gamma=1,min_child_weight=1, max_bin=128,tree_method = 'exact',subsample=0.8), nrounds = 200, disable_default_eval_metric = 1, verbose = FALSE)
predictive_biomarker_imp(model)
# Prepare the data
X_feature=tutorial_data[,c("x1", "x2" ,"x3" ,"x4","x5","x6","x7","x8","x9","x10")]
y_label=tutorial_data$y.time
delta=tutorial_data$y.event
# Convert treatment variable to (1 -1) format (1 for treatment, -1 for control)
trt=ifelse(tutorial_data$treatment==1, 1, -1)
# Estimate the propensity score using logistic regression
data_logit=tutorial_data[,c("treatment","x1", "x2" ,"x3" ,"x4" , "x5", "x6" , "x7","x8","x9","x10")]
logit_model <- glm(treatment~ ., data = data_logit, family = binomial)
pi <- predict(logit_model, type = "response")
# Train the XGBoostSub_sur model using the specified parameters
model <- XGBoostSub_sur(X_feature, y_label, X_train_trt ,pi_train,delta,Loss_type = "A_learning", params=list(learning_rate = 0.005, max_depth = 1,lambda = 9, gamma=1, min_child_weight=1,
max_bin=128, tree_method = 'exact',subsample=0.8), nrounds = 250, disable_default_eval_metric = 1, verbose = TRUE)
# Prepare the data
X_feature=tutorial_data[,c("x1", "x2" ,"x3" ,"x4","x5","x6","x7","x8","x9","x10")]
y_label=tutorial_data$y.time
delta=tutorial_data$y.event
# Convert treatment variable to (1 -1) format (1 for treatment, -1 for control)
trt=ifelse(tutorial_data$treatment==1, 1, -1)
# Estimate the propensity score using logistic regression
data_logit=tutorial_data[,c("treatment","x1", "x2" ,"x3" ,"x4" , "x5", "x6" , "x7","x8","x9","x10")]
logit_model <- glm(treatment~ ., data = data_logit, family = binomial)
pi <- predict(logit_model, type = "response")
# Train the XGBoostSub_sur model using the specified parameters
model <- XGBoostSub_sur(X_feature, y_label, trt ,pi,delta,Loss_type = "A_learning", params=list(learning_rate = 0.005, max_depth = 1,lambda = 9, gamma=1, min_child_weight=1,
max_bin=128, tree_method = 'exact',subsample=0.8), nrounds = 250, disable_default_eval_metric = 1, verbose = TRUE)
# Prepare the data
X_feature=tutorial_data[,c("x1", "x2" ,"x3" ,"x4","x5","x6","x7","x8","x9","x10")]
y_label=tutorial_data$y.time
delta=tutorial_data$y.event
# Convert treatment variable to (1 -1) format (1 for treatment, -1 for control)
trt=ifelse(tutorial_data$treatment==1, 1, -1)
# Estimate the propensity score using logistic regression
data_logit=tutorial_data[,c("treatment","x1", "x2" ,"x3" ,"x4" , "x5", "x6" , "x7","x8","x9","x10")]
logit_model <- glm(treatment~ ., data = data_logit, family = binomial)
pi <- predict(logit_model, type = "response")
# Train the XGBoostSub_sur model using the specified parameters
model <- XGBoostSub_sur(X_feature, y_label, trt ,pi,delta,Loss_type = "A_learning", params=list(learning_rate = 0.005, max_depth = 1,lambda = 9, gamma=1, min_child_weight=1,
max_bin=128, tree_method = 'exact',subsample=0.8), nrounds = 250, disable_default_eval_metric = 1, verbose = FALSE)
predictive_biomarker_imp(model)
unique(tutorial_data$risk_category)
unique(tutorial_data$treatment_categorical)
res = subgrp_perf_pred(yvar="y.time", censorvar="y.event", grpvar="risk_category", grpname=c("High Risk","Intermediate Risk" ,"Low Risk"),trtvar="treatment_categorical", trtname=c("Placebo" , "Treatment"), xvars.adj=NULL,data=tutorial_data, type="s")
kable_styling(kable(x=res$comp.res.display, format="html", caption = "Test Stat (VEN as reference)"),
bootstrap_options="striped",font_size=16)
library (kableExtra)
res = subgrp_perf_pred(yvar="y.time", censorvar="y.event", grpvar="risk_category", grpname=c("High Risk","Intermediate Risk" ,"Low Risk"),trtvar="treatment_categorical", trtname=c("Placebo" , "Treatment"), xvars.adj=NULL,data=tutorial_data, type="s")
kable_styling(kable(x=res$comp.res.display, format="html", caption = "Test Stat (VEN as reference)"),
bootstrap_options="striped",font_size=16)
kable_styling(kable(x=res$group.res.display, format="html", caption = "Subgroup Stat"),
bootstrap_options="striped",font_size=16)
kable_styling(kable(x=res$group.res.display, format="html", caption = "caption"),
bootstrap_options="striped",font_size=16)
res$fig
res = subgrp_perf_pred(yvar="y.time", censorvar="y.event", grpvar="risk_category", grpname=c("High Risk","Intermediate Risk" ,"Low Risk"),trtvar="treatment_categorical", trtname=c("Placebo" , "Treatment"), xvars.adj=NULL,data=tutorial_data, type="s")
kable_styling(kable(x=res$group.res.display, format="html", caption = "Subgroup Stat"),
bootstrap_options="striped",font_size=16)
kable_styling(kable(x=res$group.res.display, format="html", caption = "caption"),
bootstrap_options="striped",font_size=16)
res$fig
library(BioPred)
library(BioPred)
rm(list=ls())
library(BioPred)
library(ggplot2)
library(xgboost)
library(survival)
library(survminer)
library (kableExtra)
head(tutorial_data)
# Prepare the data
X_feature=tutorial_data[,c("x1", "x2" ,"x3" ,"x4","x5","x6","x7","x8","x9","x10")]
y_label=tutorial_data$y.con
# Convert treatment variable to (1 -1) format (1 for treatment, -1 for control)
trt=ifelse(tutorial_data$treatment==1, 1, -1)
# Estimate the propensity score using logistic regression
data_logit=tutorial_data[,c("treatment","x1", "x2" ,"x3" ,"x4" , "x5", "x6" , "x7","x8","x9","x10")]
logit_model <- glm(treatment~ ., data = data_logit, family = binomial)
pi <- predict(logit_model, type = "response")
# Train the XGBoostSub_con model using the specified parameters
model <- XGBoostSub_con(X_feature, y_label, trt ,pi,Loss_type = "A_learning", params = list(learning_rate = 0.005, max_depth = 1, lambda = 5, tree_method = 'hist'), nrounds = 200, disable_default_eval_metric = 0, verbose = FALSE)
predictive_biomarker_imp(model)
subgroup_results=get_subgroup_results(model, X_feature, subgroup_label=tutorial_data$subgroup_label, cutoff = 0.5)
subgroup_results=get_subgroup_results(model, X_feature, subgroup_label=tutorial_data$subgroup_label, cutoff = 0.5)
head(subgroup_results$assignment)
subgroup_results=get_subgroup_results(model, X_feature, subgroup_label=tutorial_data$subgroup_label, cutoff = 0.5)
head(subgroup_results$assignment)
cat("Prediction accuracy of true subgroup label:\n")
cat(subgroup_results$acc)
cdf_plot (xvar="x2", data=tutorial_data, y.int=5, xlim=NULL, xvar.display="Biomarker_X2", group=NULL)
scat_cont_plot(
yvar='y.con',
xvar='x2',
data=tutorial_data,
ybreaks = NULL,
xbreaks = NULL,
yvar.display = 'y',
xvar.display = "Biomarker_X2"
)
fixcut_con(yvar='y.con', xvar="x2", dir=">", cutoffs=c(-0.1,-0.3,-0.5,0.1,0.3,0.5), data=tutorial_data, method="t.test", yvar.display='y.con', xvar.display='biomarker x2', vert.x=F)
# Prepare the data
X_feature=tutorial_data[,c("x1", "x2" ,"x3" ,"x4","x5","x6","x7","x8","x9","x10")]
library(knitr)
library (kableExtra)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
# Prepare the data
X_feature=tutorial_data[,c("x1", "x2" ,"x3" ,"x4","x5","x6","x7","x8","x9","x10")]
library(BioPred)
library(knitr)
library (kableExtra)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(BioPred)
head(tutorial_data)
# Prepare the data
X_feature=tutorial_data[,c("x1", "x2" ,"x3" ,"x4","x5","x6","x7","x8","x9","x10")]
y_label=tutorial_data$y.con
# Convert treatment variable to (1 -1) format (1 for treatment, -1 for control)
trt=ifelse(tutorial_data$treatment==1, 1, -1)
# Estimate the propensity score using logistic regression
data_logit=tutorial_data[,c("treatment","x1", "x2" ,"x3" ,"x4" , "x5", "x6" , "x7","x8","x9","x10")]
logit_model <- glm(treatment~ ., data = data_logit, family = binomial)
pi <- predict(logit_model, type = "response")
# Train the XGBoostSub_con model using the specified parameters
model <- XGBoostSub_con(X_feature, y_label, trt ,pi,Loss_type = "A_learning", params = list(learning_rate = 0.005, max_depth = 1, lambda = 5, tree_method = 'hist'), nrounds = 200, disable_default_eval_metric = 0, verbose = FALSE)
# Prepare the data for training the XGBoostSub_bin
y_label=tutorial_data$y.bin
# Train the XGBoostSub_bin model using the specified parameters
model <- XGBoostSub_bin(X_feature, y_label, trt ,pi,Loss_type = "A_learning", params = list(learning_rate = 0.005, max_depth = 1, lambda = 5, tree_method = 'hist'), nrounds = 50, disable_default_eval_metric = 0, verbose = TRUE)
# Prepare the data for training the XGBoostSub_bin
y_label=tutorial_data$y.bin
# Train the XGBoostSub_bin model using the specified parameters
model <- XGBoostSub_bin(X_feature, y_label, trt ,pi,Loss_type = "A_learning", params = list(learning_rate = 0.005, max_depth = 1, lambda = 5, tree_method = 'hist'), nrounds = 300, disable_default_eval_metric = 0, verbose = TRUE)
predictive_biomarker_imp(model)
# Prepare the data for training the XGBoostSub_bin
y_label=tutorial_data$y.bin
# Train the XGBoostSub_bin model using the specified parameters
model <- XGBoostSub_bin(X_feature, y_label, trt ,pi,Loss_type = "A_learning", params = list(learning_rate = 0.01, max_depth = 1, lambda = 5, tree_method = 'hist'), nrounds = 500, disable_default_eval_metric = 0, verbose = TRUE)
predictive_biomarker_imp(model)
# Prepare the data for training the XGBoostSub_bin
y_label=tutorial_data$y.bin
# Train the XGBoostSub_bin model using the specified parameters
model <- XGBoostSub_bin(X_feature, y_label, trt ,pi,Loss_type = "A_learning", params = list(learning_rate = 0.01, max_depth = 1, lambda = 5, tree_method = 'hist'), nrounds = 300, disable_default_eval_metric = 0, verbose = TRUE)
predictive_biomarker_imp(model)
# Prepare the data for training the XGBoostSub_bin
y_label=tutorial_data$y.bin
# Train the XGBoostSub_bin model using the specified parameters
model <- XGBoostSub_bin(X_feature, y_label, trt ,pi,Loss_type = "A_learning", params = list(learning_rate = 0.01, max_depth = 1, lambda = 5, tree_method = 'hist'), nrounds = 300, disable_default_eval_metric = 0, verbose = FALSE)
subgroup_results=get_subgroup_results(model, X_feature, subgroup_label=tutorial_data$subgroup_label, cutoff = 0.5)
head(subgroup_results$assignment)
cat("Prediction accuracy of true subgroup label:\n")
cat(subgroup_results$acc)
roc_bin_plot (yvar='y.bin', xvars="x10", dirs="auto", data=tutorial_data, yvar.display='y.bin', xvars.display="Biomarker x10")
install.packages("pROC")
library(BioPred)
head(tutorial_data)
# Prepare the data
X_feature=tutorial_data[,c("x1", "x2" ,"x3" ,"x4","x5","x6","x7","x8","x9","x10")]
y_label=tutorial_data$y.con
# Convert treatment variable to (1 -1) format (1 for treatment, -1 for control)
trt=ifelse(tutorial_data$treatment==1, 1, -1)
# Estimate the propensity score using logistic regression
data_logit=tutorial_data[,c("treatment","x1", "x2" ,"x3" ,"x4" , "x5", "x6" , "x7","x8","x9","x10")]
logit_model <- glm(treatment~ ., data = data_logit, family = binomial)
pi <- predict(logit_model, type = "response")
# Train the XGBoostSub_con model using the specified parameters
model <- XGBoostSub_con(X_feature, y_label, trt ,pi,Loss_type = "A_learning", params = list(learning_rate = 0.005, max_depth = 1, lambda = 5, tree_method = 'hist'), nrounds = 200, disable_default_eval_metric = 0, verbose = FALSE)
roc_bin_plot (yvar='y.bin', xvars="x10", dirs="auto", data=tutorial_data, yvar.display='y.bin', xvars.display="Biomarker x10")
fixcut_bin (yvar='y.bin', xvar="x10", dir=">", cutoffs=c(-0.1,-0.3,-0.5, 0.1,0.3,0.5), data=tutorial_data, method="Fisher", yvar.display='Response_Y', xvar.display='Biomarker_X10', vert.x=F)
usethis::edit_r_environ()
?base::complete.cases
??base::complete.cases
??complete.cases
?complete.cases
??xgboost::predict.xgb.Booster
xgboost::predict
install.packages("mlr")
xgboost::predict
??xgboost::predict
X_feature=tutorial_data[,c("x1", "x2" ,"x3" ,"x4","x5","x6","x7","x8","x9","x10")]
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(knitr)
library (kableExtra)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(knitr)
library (kableExtra)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(BioPred)
head(tutorial_data)
X_feature=tutorial_data[,c("x1", "x2" ,"x3" ,"x4","x5","x6","x7","x8","x9","x10")]
y_label=tutorial_data$y.con
# Convert treatment variable to (1 -1) format (1 for treatment, -1 for control)
trt=ifelse(tutorial_data$treatment==1, 1, -1)
true_label=tutorial_data$subgroup_label
# Estimate the propensity score using logistic regression
data_logit=tutorial_data[,c("treatment","x1", "x2" ,"x3" ,"x4" , "x5", "x6" , "x7","x8","x9","x10")]
logit_model <- glm(treatment~ ., data = data_logit, family = binomial)
pi <- predict(logit_model, type = "response")
# Train the XGBoostSub_con model using the specified parameters
model <- XGBoostSub_con(X_feature, y_label, trt ,pi,Loss_type = "A_learning", params = list(learning_rate = 0.005, max_depth = 1, lambda = 5, tree_method = 'hist'), nrounds = 200, disable_default_eval_metric = 0, verbose = FALSE)
predictive_biomarker_imp(model)
biomarker_imp=predictive_biomarker_imp(model)
kable_styling(kable(x=biomarker_imp, format="html", caption = "caption"),
bootstrap_options="striped",font_size=16)
subgroup_results=get_subgroup_results(model, X_feature, subgroup_label=true_label, cutoff = 0.5)
cat('the first 6 patienst subgroup assigentment')
head(subgroup_results$assignment)
cat("Prediction accuracy of true subgroup label:\n")
kable_styling(kable(x=subgroup_results$acc, format="html", caption = "caption"),
bootstrap_options="striped",font_size=16)
subgroup_results=get_subgroup_results(model, X_feature, subgroup_label=true_label, cutoff = 0.5)
cat('the first 6 patienst subgroup assigentment')
kable_styling(kable(x=head(subgroup_results$assignment), format="html", caption = "the first 6 patienst subgroup assigentment"),
bootstrap_options="striped",font_size=16)
cat("Prediction accuracy of true subgroup label:\n")
cat(subgroup_results$acc)
subgroup_results=get_subgroup_results(model, X_feature, subgroup_label=true_label, cutoff = 0.5)
kable_styling(kable(x=head(subgroup_results$assignment), format="html", caption = "the first 6 patienst subgroup assigentment"),
bootstrap_options="striped",font_size=16)
cat("Prediction accuracy of true subgroup label:\n")
cat(subgroup_results$acc)
library(BioPred)
kable_styling(kable(x=head(tutorial_data), format="html", caption = "the first 6 subjects"),
bootstrap_options="striped",font_size=16)
rm(list=ls())
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(knitr)
library (kableExtra)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(BioPred)
kable_styling(kable(x=head(tutorial_data), format="html", caption = "The first 6 subjects"),
bootstrap_options="striped",font_size=16)
X_feature=tutorial_data[,c("x1", "x2" ,"x3" ,"x4","x5","x6","x7","x8","x9","x10")]
y_label=tutorial_data$y.con
# Convert treatment variable to (1 -1) format (1 for treatment, -1 for control)
trt=ifelse(tutorial_data$treatment==1, 1, -1)
true_label=tutorial_data$subgroup_label
# Estimate the propensity score using logistic regression
data_logit=tutorial_data[,c("treatment","x1", "x2" ,"x3" ,"x4" , "x5", "x6" , "x7","x8","x9","x10")]
logit_model <- glm(treatment~ ., data = data_logit, family = binomial)
pi <- predict(logit_model, type = "response")
# Train the XGBoostSub_con model using the specified parameters
model <- XGBoostSub_con(X_feature, y_label, trt ,pi,Loss_type = "A_learning", params = list(learning_rate = 0.005, max_depth = 1, lambda = 5, tree_method = 'hist'), nrounds = 200, disable_default_eval_metric = 0, verbose = FALSE)
cat("Evaluation loss:\n")
print(model$evaluation_log)
eval_metric_test <- eval_metric_con(model, X_feature, y_label, pi, trt, Loss_type = "A_learning")
cat("Testing Evaluation Result:\n")
print(eval_metric_test$value)
biomarker_imp=predictive_biomarker_imp(model)
kable_styling(kable(x=biomarker_imp, format="html", caption = "Biomarker importance table"),
bootstrap_options="striped",font_size=16)
subgroup_results=get_subgroup_results(model, X_feature, subgroup_label=true_label, cutoff = 0.5)
kable_styling(kable(x=head(subgroup_results$assignment), format="html", caption = "The first 6 subjects subgroup assigentment"),
bootstrap_options="striped",font_size=16)
cat("Prediction accuracy of true subgroup label:\n")
cat(subgroup_results$acc)
cdf_plot (xvar="x2", data=tutorial_data, y.int=5, xlim=NULL, xvar.display="Biomarker_X2", group=NULL)
scat_cont_plot(
yvar='y.con',
xvar='x2',
data=tutorial_data,
ybreaks = NULL,
xbreaks = NULL,
yvar.display = 'y',
xvar.display = "Biomarker_X2"
)
library(BioPred)
library(BioPred)
devtools::check()
R CMD check brocolors_0.1.tar.gz
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(BioPred)
kable_styling(kable(x=head(tutorial_data), format="html", caption = "The first 6 subjects"),
bootstrap_options="striped",font_size=16)
library(knitr)
library (kableExtra)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(BioPred)
kable_styling(kable(x=head(tutorial_data), format="html", caption = "The first 6 subjects"),
bootstrap_options="striped",font_size=16)
rownames(tutorial_data)
rownames(tutorial_data)=NULL
rownames(tutorial_data)
library(BioPred)
kable_styling(kable(x=head(tutorial_data), format="html", caption = "The first 6 subjects"),
bootstrap_options="striped",font_size=16)
rownames(tutorial_data)=NULL
yvar.display='y.con'
xvar.display='biomarker x2'
yvar='y.con'
xvar="x2"
dir=">"
data=tutorial_data
Cutoff <- NULL
Mean <- NULL
Group <- NULL
Mean.Lo <- NULL
Mean.Up <- NULL
data <- data[, c(xvar, yvar)]
data <- data[stats::complete.cases(data),]
rownames(data)=NULL
data=tutorial_data
Cutoff <- NULL
Mean <- NULL
Group <- NULL
Mean.Lo <- NULL
Mean.Up <- NULL
data <- data[, c(xvar, yvar)]
data <- data[stats::complete.cases(data),]
View(data)
if ("Group" %in% names(data)) stop("xvar and yvar should not have the name of Group.")
if (!dir %in% c(">", "<", ">=", "<=")) stop("dir should be >, <, >=, or <=")
cutoffs <- sort(unique(cutoffs))
stat <- NULL
cutoffs=c(-0.1,-0.3,-0.5,0.1,0.3,0.5)
if ("Group" %in% names(data)) stop("xvar and yvar should not have the name of Group.")
if (!dir %in% c(">", "<", ">=", "<=")) stop("dir should be >, <, >=, or <=")
cutoffs <- sort(unique(cutoffs))
stat <- NULL
for (i in seq_along(cutoffs)) {
if (dir == ">") {
data$Group <- (data[, xvar] > cutoffs[i]) * 1
} else if (dir == ">=") {
data$Group <- (data[, xvar] >= cutoffs[i]) * 1
} else if (dir == "<") {
data$Group <- (data[, xvar] < cutoffs[i]) * 1
} else if (dir == "<=") {
data$Group <- (data[, xvar] <= cutoffs[i]) * 1
}
fml <- stats::as.formula(paste(yvar, "~ Group"))
ttest <- try(stats::t.test(fml, data = data, alternative = "two.sided", mu = 0, paired = FALSE, var.equal = FALSE, conf.level = 0.95), silent = TRUE)
if (!inherits(ttest, "try-error")) {
pval.t <- ttest$p.value
mean.diff.lo <- ttest$conf.int[1]
mean.diff.up <- ttest$conf.int[2]
} else {
pval.t <- NA
mean.diff.lo <- NA
mean.diff.up <- NA
}
group0 <- data[data$Group == 0, ]
group1 <- data[data$Group == 1, ]
group0.N <- nrow(group0)
group1.N <- nrow(group1)
group0.t <- try(stats::t.test(group0[, yvar], alternative = "two.sided", mu = 0, conf.level = 0.95), silent = TRUE)
group1.t <- try(stats::t.test(group1[, yvar], alternative = "two.sided", mu = 0, conf.level = 0.95), silent = TRUE)
if (!inherits(group0.t, "try-error")) {
group0.mean <- unname(group0.t$estimate)
group0.lower <- group0.t$conf.int[1]
group0.upper <- group0.t$conf.int[2]
group0.median <- stats::median(group0[, yvar])
} else {
group0.mean <- mean(group0[, yvar], na.rm = TRUE)
group0.lower <- NA
group0.upper <- NA
group0.median <- stats::median(group0[, yvar], na.rm = TRUE)
}
if (!inherits(group1.t, "try-error")) {
group1.mean <- unname(group1.t$estimate)
group1.lower <- group1.t$conf.int[1]
group1.upper <- group1.t$conf.int[2]
group1.median <- stats::median(group1[, yvar])
} else {
group1.mean <- mean(group1[, yvar], na.rm = TRUE)
group1.lower <- NA
group1.upper <- NA
group1.median <- stats::median(group1[, yvar], na.rm = TRUE)
}
mean.diff <- group0.mean - group1.mean
median.diff <- group0.median - group1.median
stat <- rbind(stat, data.frame(Response = yvar.display, Predictor = xvar.display,
Direction = dir, Cutoff = cutoffs[i], Mean.Diff = mean.diff,
Mean.Diff.Lo = mean.diff.lo, Mean.Diff.Up = mean.diff.up,
ttest.pval = pval.t, Median.Diff = median.diff,
Group0.N = group0.N, Group0.Mean = group0.mean,
Group0.Mean.Lo = group0.lower, Group0.Mean.Up = group0.upper,
Group0.Median = group0.median,
Group1.N = group1.N, Group1.Mean = group1.mean,
Group1.Mean.Lo = group1.lower, Group1.Mean.Up = group1.upper,
Group1.Median = group1.median))
}
View(stat)
stat.display <- stat
stat.display <- stat.display[, c("Response", "Predictor", "Direction", "Cutoff", "Group0.N", "Group1.N",
"Median.Diff", "Mean.Diff", "Mean.Diff.Lo", "Mean.Diff.Up", "ttest.pval")]
temp <- stat.display[, c("Median.Diff", "Mean.Diff", "Mean.Diff.Lo", "Mean.Diff.Up")]
stat.display[, c("Median.Diff", "Mean.Diff", "Mean.Diff.Lo", "Mean.Diff.Up")] <- round(temp, digits = 2)
stat.display$ttest.pval <- round(stat.display$ttest.pval, digits = 4)
stat.display$Mean.Diff.CI <- paste(stat.display$Mean.Diff, " (", stat.display$Mean.Diff.Lo, " , ",
stat.display$Mean.Diff.Up, ")", sep = "")
stat.display <- stat.display[, c("Response", "Predictor", "Direction", "Cutoff", "Group0.N", "Group1.N",
"Median.Diff", "Mean.Diff.CI", "ttest.pval")]
if (method == "t.test") {
stat.sel <- stat[which.min(stat$ttest.pval), ]
stat.sel.display <- stat.display[which.min(stat$ttest.pval), ]
}
cut.sel <- stat.sel$Cutoff
method="t.test"
stat.display <- stat
stat.display <- stat.display[, c("Response", "Predictor", "Direction", "Cutoff", "Group0.N", "Group1.N",
"Median.Diff", "Mean.Diff", "Mean.Diff.Lo", "Mean.Diff.Up", "ttest.pval")]
temp <- stat.display[, c("Median.Diff", "Mean.Diff", "Mean.Diff.Lo", "Mean.Diff.Up")]
stat.display[, c("Median.Diff", "Mean.Diff", "Mean.Diff.Lo", "Mean.Diff.Up")] <- round(temp, digits = 2)
stat.display$ttest.pval <- round(stat.display$ttest.pval, digits = 4)
stat.display$Mean.Diff.CI <- paste(stat.display$Mean.Diff, " (", stat.display$Mean.Diff.Lo, " , ",
stat.display$Mean.Diff.Up, ")", sep = "")
stat.display <- stat.display[, c("Response", "Predictor", "Direction", "Cutoff", "Group0.N", "Group1.N",
"Median.Diff", "Mean.Diff.CI", "ttest.pval")]
if (method == "t.test") {
stat.sel <- stat[which.min(stat$ttest.pval), ]
stat.sel.display <- stat.display[which.min(stat$ttest.pval), ]
}
cut.sel <- stat.sel$Cutoff
View(stat.sel)
View(stat.sel.display)
View(stat.sel)
View(stat.display)
View(stat.display)
View(stat.sel.display)
View(stat.display)
View(stat.sel.display)
rownames(stat.sel) = NULL
rownames(stat.sel.display) = NULL
rownames(stat.display) = NULL
if (dir == "<") {
dir.else <- ">="
} else if (dir == "<=") {
dir.else <- ">"
} else if (dir == ">") {
dir.else <- "<="
} else if (dir == ">=") {
dir.else <- "<"
}
group0.res <- stat.sel[, c("Response", "Group0.N", "Group0.Mean", "Group0.Mean.Lo", "Group0.Mean.Up", "Group0.Median")]
group1.res <- stat.sel[, c("Response", "Group1.N", "Group1.Mean", "Group1.Mean.Lo", "Group1.Mean.Up", "Group1.Median")]
View(group1.res)
View(group1)
View(group1.res)
names(group0.res) <- c("Response", "N", "Mean", "Mean.Lo", "Mean.Up", "Median")
View(group0.res)
group0.res <- stat.sel[, c("Response", "Group0.N", "Group0.Mean", "Group0.Mean.Lo", "Group0.Mean.Up", "Group0.Median")]
View(group0.res)
colnames(group0.res) <- c("Response", "N", "Mean", "Mean.Lo", "Mean.Up", "Median")
colnames(group0.res) <- c("Response", "N", "Mean", "Mean.Lo", "Mean.Up", "Median")
colnames(group1.res) <- c("Response", "N", "Mean", "Mean.Lo", "Mean.Up", "Median")
group.res <- rbind(group0.res, group1.res)
View(group.res)
rownames(group.res) <- c(paste(xvar.display, dir.else, cut.sel), paste(xvar.display, dir, cut.sel))
c(paste(xvar.display, dir.else, cut.sel), paste(xvar.display, dir, cut.sel))
data.plot0 <- stat[, c("Cutoff", "Group0.Mean", "Group0.Mean.Lo", "Group0.Mean.Up")]
data.plot0$Group <- paste(dir.else, "Cutoff")
View(data.plot0)
data.plot0 <- stat[, c("Cutoff", "Group0.Mean", "Group0.Mean.Lo", "Group0.Mean.Up")]
data.plot0$Group <- paste(dir.else, "Cutoff")
colnames(data.plot0) <- c("Cutoff", "Mean", "Mean.Lo", "Mean.Up", "Group")
data.plot1 <- stat[, c("Cutoff", "Group1.Mean", "Group1.Mean.Lo", "Group1.Mean.Up")]
data.plot1$Group <- paste(dir, "Cutoff")
colnames(data.plot1) <- c("Cutoff", "Mean", "Mean.Lo", "Mean.Up", "Group")
data.plot <- rbind(data.plot0, data.plot1)
View(data.plot)
View(group.res)
rownames(group.res)
length(rownames(group.res))
data <- data[, c(xvar, yvar)]
data <- data[stats::complete.cases(data),]
row.names(data)=NULL
View(data)
row.names(group.res)
rm(list=ls())
library(BioPred)
library(BioPred)
usethis::use_testthat()
library(BioPred)
devtools::check_rhub(pkg = ``. '', platforms = `debian-gcc-devel-nold'' )
R --version
R.version.string
git remote add origin https://github.com/deeplearner0731/BioPred
